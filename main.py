import requests
from bs4 import BeautifulSoup
import datetime
import urllib.parse
import time
import random

# ================= é…ç½®åŒºåŸŸ =================
# æ ¸å¿ƒæœç´¢è¯
SUBJECT = "é«˜ä¸­åŒ–å­¦"

# æ„é€ å¾®ä¿¡å…¬ä¼—å·ä¸“å±æœç´¢æŒ‡ä»¤
# æ ¼å¼ï¼šå…³é”®è¯ + site:mp.weixin.qq.com
SEARCH_TASKS = {
    "ğŸ“¢ ä¼˜è´¨è¯¾/å…¬å¼€è¯¾ (å…¬ä¼—å·)": [
        f"{SUBJECT} ä¼˜è´¨è¯¾å¤§èµ› ä¸€ç­‰å¥– site:mp.weixin.qq.com",
        f"{SUBJECT} è¯¾å ‚å®å½• æ•™å­¦è®¾è®¡ site:mp.weixin.qq.com",
        f"{SUBJECT} è¯´è¯¾ç¨¿ site:mp.weixin.qq.com"
    ],
    "ğŸ“ æ•™å­¦è®ºæ–‡/å¹²è´§ (å…¬ä¼—å·)": [
        f"{SUBJECT} æ ¸å¿ƒç´ å…» è®ºæ–‡ site:mp.weixin.qq.com",
        f"{SUBJECT} å¤§å•å…ƒæ•™å­¦ æ¡ˆä¾‹ site:mp.weixin.qq.com",
        f"{SUBJECT} é«˜è€ƒå¤‡è€ƒç­–ç•¥ site:mp.weixin.qq.com"
    ],
    "ğŸ’¡ ç­ä¸»ä»»/åå¸ˆå·¥ä½œå®¤ (å…¬ä¼—å·)": [
        f"{SUBJECT} åå¸ˆå·¥ä½œå®¤ æ•™å­¦åæ€ site:mp.weixin.qq.com",
        f"é«˜ä¸­ç­ä¸»ä»» å¾·è‚²æ¡ˆä¾‹ site:mp.weixin.qq.com",
        f"åŒ–å­¦è€å¸ˆ æ•™å­¦æ„Ÿæ‚Ÿ site:mp.weixin.qq.com"
    ]
}

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept-Language': 'zh-CN,zh;q=0.9'
}
# ===========================================

def fetch_wechat_via_bing(query):
    """é€šè¿‡ Bing æœç´¢å®šå‘æŠ“å–å¾®ä¿¡å…¬ä¼—å·æ–‡ç« """
    try:
        # Bing æœç´¢ URL
        url = f"https://www.bing.com/search?q={urllib.parse.quote(query)}"
        response = requests.get(url, headers=HEADERS, timeout=15)
        soup = BeautifulSoup(response.text, 'lxml')
        
        results = []
        # Bing ç»“æœåˆ—è¡¨
        items = soup.select('li.b_algo')
        
        for item in items[:5]: # æ¯ä¸ªè¯æŠ“å‰5æ¡
            h2 = item.find('h2')
            if not h2: continue
            
            link_tag = h2.find('a')
            if not link_tag: continue
            
            title = link_tag.get_text().strip()
            href = link_tag['href']
            
            # è¿‡æ»¤æ‰éå¾®ä¿¡åŸŸåçš„æ‚è´¨ (è™½ç„¶ç”¨äº† site æŒ‡ä»¤ï¼Œç”±äºå¹¿å‘ŠåŸå› å¶å°”ä¼šæœ‰æ¼ç½‘ä¹‹é±¼)
            # æ³¨æ„ï¼šBing å¯èƒ½ä¼šå¯¹é“¾æ¥è¿›è¡Œè·³è½¬å¤„ç†ï¼Œè¿™é‡Œæˆ‘ä»¬å°½é‡æŠ“å–
            
            # æå–æ‘˜è¦
            snippet_tag = item.select_one('.b_caption p')
            snippet = snippet_tag.get_text().strip()[:80] + "..." if snippet_tag else "ç‚¹å‡»é˜…è¯»å…¨æ–‡"
            
            # æå–å‘å¸ƒæ—¶é—´ (å°è¯•ä»æ‘˜è¦ä¸­æå–æ—¥æœŸï¼Œä¾‹å¦‚ "2å¤©å‰", "2023-10-1")
            # Bing çš„æ—¥æœŸé€šå¸¸åœ¨ä¸€ä¸ª span class="news_dt" æˆ–è€…æ‘˜è¦å¼€å¤´
            date_tag = item.select_one('span.news_dt')
            date = date_tag.get_text().strip() if date_tag else "è¿‘æœŸ"

            results.append({
                "title": title,
                "link": href,
                "snippet": snippet,
                "date": date
            })
        return results
    except Exception as e:
        print(f"æœç´¢ '{query}' å¤±è´¥: {e}")
        return []

def generate_markdown(data_dict):
    md = f"# ğŸŸ¢ å¾®ä¿¡å…¬ä¼—å·ç²¾é€‰æ—¥æŠ¥\n\n"
    md += f"**ç”Ÿæˆæ—¶é—´**: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')} (UTC)\n"
    md += "> æœ¬æ—¥æŠ¥å®šå‘æŠ“å–å¾®ä¿¡å…¬ä¼—å·ï¼ˆmp.weixin.qq.comï¼‰å‘å¸ƒçš„ä¼˜è´¨æ•™å­¦èµ„æºã€‚\n\n"

    for category, items in data_dict.items():
        md += f"## {category}\n\n"
        if not items:
            md += "ä»Šæ—¥æœªæŠ“å–åˆ°ç›¸å…³å†…å®¹ã€‚\n\n"
            continue
            
        md += "| æ–‡ç« æ ‡é¢˜ | æ‘˜è¦é¢„è§ˆ | å‘å¸ƒ/æ”¶å½•æ—¶é—´ |\n"
        md += "|---|---|---|\n"
        
        seen_links = set()
        
        for item in items:
            # ç®€å•å»é‡
            if item['link'] in seen_links: continue
            seen_links.add(item['link'])
            
            # æ ¼å¼åŒ–æ ‡é¢˜
            clean_title = item['title'].replace('|', '-').replace(' - å¾®ä¿¡å…¬ä¼—å¹³å°', '').replace('mp.weixin.qq.com', '')
            clean_snippet = item['snippet'].replace('|', '/')
            
            md += f"| [ğŸ“„ {clean_title}]({item['link']}) | {clean_snippet} | {item['date']} |\n"
        md += "\n"
        
    return md

def main():
    all_resources = {}
    
    print("ğŸš€ å¼€å§‹æŠ“å–å¾®ä¿¡å…¬ä¼—å·å†…å®¹...")
    
    for category, queries in SEARCH_TASKS.items():
        print(f"\nğŸ“‚ æ­£åœ¨å¤„ç†: {category}")
        category_results = []
        
        for query in queries:
            print(f"  ğŸ” æœç´¢æŒ‡ä»¤: {query}")
            res = fetch_wechat_via_bing(query)
            category_results.extend(res)
            # éšæœºå»¶æ—¶ 2-5 ç§’ï¼Œé¿å… Bing è®¤ä¸ºæˆ‘ä»¬æ˜¯æœºå™¨äºº
            time.sleep(random.uniform(2, 5))
            
        all_resources[category] = category_results

    content = generate_markdown(all_resources)
    with open("README.md", "w", encoding="utf-8") as f:
        f.write(content)
    print("\nâœ… æŠ“å–å®Œæˆï¼è¯·æŸ¥çœ‹ README.md")

if __name__ == "__main__":
    main()
