import requests
from bs4 import BeautifulSoup
import datetime
import urllib.parse
import time
import random
import re

# ================= é…ç½®åŒºåŸŸ =================
# åŸºç¡€å­¦ç§‘è¯
SUBJECT = "é«˜ä¸­åŒ–å­¦"

# å®šä¹‰ä¸‰ä¸ªç»´åº¦çš„æœç´¢ä»»åŠ¡
SEARCH_TASKS = {
    "ğŸ“„ æ•™å­¦è®ºæ–‡/æ•™æ¡ˆ (æ–‡æ¡£)": [
        f"{SUBJECT} æ ¸å¿ƒç´ å…» æ•™å­¦è®ºæ–‡ filetype:pdf",
        f"{SUBJECT} å¤§å•å…ƒæ•™å­¦è®¾è®¡ filetype:doc",
        f"{SUBJECT} å®éªŒæ”¹è¿› è®ºæ–‡ filetype:pdf"
    ],
    "ğŸ’¡ æ•™å­¦å¿ƒå¾—/åæ€ (çŸ¥ä¹/ç»éªŒ)": [
        f"site:zhihu.com {SUBJECT} æ•™å­¦åæ€",
        f"site:zhihu.com {SUBJECT} ç­ä¸»ä»»ç®¡ç†ç»éªŒ",
        f"{SUBJECT} æ•™å­¦ä¸­çš„å›°æƒ‘ä¸å¯¹ç­–"
    ],
    "ğŸ« ç»å…¸æ•™å­¦æ¡ˆä¾‹": [
        f"{SUBJECT} ä¼˜è´¨è¯¾ æ•™å­¦è®¾è®¡",
        f"{SUBJECT} è¯¾ç¨‹æ€æ”¿ æ•™å­¦æ¡ˆä¾‹",
        f"{SUBJECT} æ¢ç©¶å¼æ•™å­¦ æ¡ˆä¾‹åˆ†æ"
    ]
}

# æ¨¡æ‹Ÿæµè§ˆå™¨å¤´
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept-Language': 'zh-CN,zh;q=0.9'
}
# ===========================================

def fetch_bing_search(query):
    """åˆ©ç”¨ Bing æœç´¢è·å–ç»“æœ (æœ€é€‚åˆæœæ–‡ä»¶å’ŒçŸ¥ä¹)"""
    try:
        url = f"https://www.bing.com/search?q={urllib.parse.quote(query)}"
        response = requests.get(url, headers=HEADERS, timeout=15)
        soup = BeautifulSoup(response.text, 'lxml')
        
        results = []
        # Bing çš„æœç´¢ç»“æœé€šå¸¸åœ¨ li.b_algo ä¸­
        items = soup.select('li.b_algo')
        
        for item in items[:4]: # æ¯ä¸ªå…³é”®è¯å–å‰4æ¡
            h2 = item.find('h2')
            if not h2: continue
            
            link_tag = h2.find('a')
            if not link_tag: continue
            
            title = link_tag.get_text().strip()
            href = link_tag['href']
            
            # æå–æ‘˜è¦
            snippet_tag = item.select_one('.b_caption p')
            snippet = snippet_tag.get_text().strip()[:60] + "..." if snippet_tag else "æš‚æ— æ‘˜è¦"
            
            results.append({
                "title": title,
                "link": href,
                "snippet": snippet,
                "engine": "Bing"
            })
        return results
    except Exception as e:
        print(f"[Bing] æœç´¢ '{query}' å¤±è´¥: {e}")
        return []

def fetch_baidu_search(query):
    """åˆ©ç”¨ ç™¾åº¦ æœç´¢è·å–ç»“æœ (é€‚åˆæœå›½å†…ä¸€èˆ¬æ–‡ç« )"""
    try:
        url = f"https://www.baidu.com/s?wd={urllib.parse.quote(query)}"
        response = requests.get(url, headers=HEADERS, timeout=15)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        results = []
        # ç™¾åº¦æ™®é€šæœç´¢ç»“æœå®¹å™¨
        items = soup.select('div.result.c-container')
        
        for item in items[:4]:
            h3 = item.find('h3')
            if not h3: continue
            link_tag = h3.find('a')
            if not link_tag: continue
            
            title = link_tag.get_text().strip()
            href = link_tag['href']
            
            # ç™¾åº¦æ‘˜è¦
            # ç™¾åº¦ç»“æ„å¤æ‚ï¼Œå°è¯•å¤šç§é€‰æ‹©å™¨
            snippet = "ç‚¹å‡»æŸ¥çœ‹è¯¦æƒ…"
            abstract_tag = item.select_one('.c-abstract') or item.select_one('.content-right_8Zs40')
            if abstract_tag:
                snippet = abstract_tag.get_text().strip()[:60] + "..."

            results.append({
                "title": title,
                "link": href,
                "snippet": snippet,
                "engine": "Baidu"
            })
        return results
    except Exception as e:
        print(f"[Baidu] æœç´¢ '{query}' å¤±è´¥: {e}")
        return []

def generate_markdown(data_dict):
    md = f"# ğŸ§ª é«˜ä¸­åŒ–å­¦æ•™è‚²èµ„æºæ—¥æŠ¥\n\n"
    md += f"**ç”Ÿæˆæ—¶é—´**: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')} (UTC)\n"
    md += "> æœ¬æ—¥æŠ¥èšåˆäº†æ•™å­¦è®ºæ–‡ã€çŸ¥ä¹å¿ƒå¾—åŠä¼˜è´¨æ•™æ¡ˆèµ„æºã€‚\n\n"

    for category, items in data_dict.items():
        md += f"## {category}\n\n"
        if not items:
            md += "ä»Šæ—¥æœªæŠ“å–åˆ°ç›¸å…³å†…å®¹ã€‚\n\n"
            continue
            
        md += "| æ ‡é¢˜ | æ‘˜è¦/å…³é”®è¯ | æ¥æº |\n"
        md += "|---|---|---|\n"
        
        seen_links = set()
        
        for item in items:
            if item['link'] in seen_links: continue
            seen_links.add(item['link'])
            
            # æ ‡è®°æ–‡ä»¶ç±»å‹
            title_prefix = ""
            if "filetype:pdf" in str(SEARCH_TASKS).lower() and item['link'].endswith('.pdf'):
                title_prefix = "ğŸ“„ [PDF] "
            elif "filetype:doc" in str(SEARCH_TASKS).lower() and item['link'].endswith('.doc'):
                title_prefix = "ğŸ“ [DOC] "
                
            clean_title = item['title'].replace('|', '-').replace('\n', '')
            clean_snippet = item['snippet'].replace('|', '/').replace('\n', '')
            
            md += f"| [{title_prefix}{clean_title}]({item['link']}) | {clean_snippet} | {item['engine']} |\n"
        md += "\n"
        
    return md

def main():
    all_resources = {}
    
    for category, queries in SEARCH_TASKS.items():
        print(f"\næ­£åœ¨å¤„ç†åˆ†ç±»: {category} ...")
        category_results = []
        
        for query in queries:
            print(f"  - æœç´¢æŒ‡ä»¤: {query}")
            
            # ç­–ç•¥ï¼šæ–‡æ¡£ç±»å’ŒçŸ¥ä¹ç±»ä¼˜å…ˆç”¨ Bingï¼Œå…¶ä»–ç”¨ Baidu
            # è¿™æ ·åšæ˜¯å› ä¸º Bing å¯¹ filetype å’Œ site æŒ‡ä»¤æ”¯æŒæ›´å¥½
            if "filetype" in query or "site:zhihu" in query:
                res = fetch_bing_search(query)
            else:
                # éšæœºé€‰æ‹©å¼•æ“ä»¥å¢åŠ ä¸°å¯Œåº¦ï¼Œæˆ–è€…åŒæ—¶æŠ“å–
                res = fetch_baidu_search(query)
                if not res: # å¦‚æœç™¾åº¦æ²¡æŠ“åˆ°ï¼ˆå¯èƒ½è¢«åçˆ¬ï¼‰ï¼Œå°è¯•ç”¨ Bing è¡¥æ•‘
                     res = fetch_bing_search(query)
            
            category_results.extend(res)
            time.sleep(random.uniform(2, 4)) # éšæœºå»¶æ—¶é˜²å°
            
        all_resources[category] = category_results

    # ç”Ÿæˆå¹¶ä¿å­˜
    content = generate_markdown(all_resources)
    with open("README.md", "w", encoding="utf-8") as f:
        f.write(content)
    print("\nèµ„æºæŠ“å–å®Œæˆï¼ŒREADME.md å·²æ›´æ–°ï¼")

if __name__ == "__main__":
    main()
