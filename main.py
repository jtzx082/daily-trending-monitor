import requests
from bs4 import BeautifulSoup
import datetime
import urllib.parse
import time
import random

# ================= é…ç½®åŒºåŸŸ =================
KEYWORDS = [
    "é«˜ä¸­åŒ–å­¦",
    "ç­ä¸»ä»»",
    "Gemini"
]

# æ¨¡æ‹ŸçœŸå®çš„æµè§ˆå™¨èº«ä»½ï¼Œé˜²æ­¢è¢«ç™¾åº¦æ‹¦æˆª
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8'
}
# ===========================================

def fetch_google_rss(keyword):
    """æŠ“å– Google News RSS"""
    try:
        # hl=zh-CN: ç•Œé¢è¯­è¨€ä¸­æ–‡, gl=CN: åœ°ç†ä½ç½®ä¸­å›½, ceid=CN:zh-Hans: åŒºåŸŸè®¾ç½®
        url = f"https://news.google.com/rss/search?q={urllib.parse.quote(keyword)}&hl=zh-CN&gl=CN&ceid=CN:zh-Hans"
        response = requests.get(url, headers=HEADERS, timeout=15)
        soup = BeautifulSoup(response.content, 'xml')
        items = soup.find_all('item')
        
        results = []
        for item in items[:5]: # å–å‰5æ¡
            results.append({
                "title": item.title.text,
                "link": item.link.text,
                "date": item.pubDate.text[:16] if item.pubDate else "",
                "source": "Google"
            })
        return results
    except Exception as e:
        print(f"[Google] æŠ“å–å¤±è´¥: {e}")
        return []

def fetch_bing_rss(keyword):
    """æŠ“å– Bing News RSS"""
    try:
        url = f"https://www.bing.com/news/search?q={urllib.parse.quote(keyword)}&format=rss"
        response = requests.get(url, headers=HEADERS, timeout=15)
        soup = BeautifulSoup(response.content, 'xml')
        items = soup.find_all('item')
        
        results = []
        for item in items[:5]:
            results.append({
                "title": item.title.text,
                "link": item.link.text,
                "date": item.pubDate.text[:16] if item.pubDate else "",
                "source": "Bing"
            })
        return results
    except Exception as e:
        print(f"[Bing] æŠ“å–å¤±è´¥: {e}")
        return []

def fetch_baidu_html(keyword):
    """æŠ“å– ç™¾åº¦èµ„è®¯ HTML (éš¾åº¦æœ€é«˜)"""
    try:
        # tn=news: æœç´¢èµ„è®¯, rtt=1: æŒ‰æ—¶é—´æ’åº(1)æˆ–ç›¸å…³æ€§(4)
        url = f"https://www.baidu.com/s?tn=news&rtt=1&bsst=1&cl=2&wd={urllib.parse.quote(keyword)}"
        
        # ç™¾åº¦å¯¹ Cookie æœ‰ä¸€å®šæ ¡éªŒï¼Œè¿™é‡ŒåªåšåŸºç¡€è¯·æ±‚ï¼Œå¦‚æœå¤±è´¥åˆ™è¿”å›ç©º
        response = requests.get(url, headers=HEADERS, timeout=15)
        
        # å°è¯•è§£å†³ä¸­æ–‡ä¹±ç é—®é¢˜
        response.encoding = 'utf-8' 
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ç™¾åº¦èµ„è®¯çš„åˆ—è¡¨é€šå¸¸åœ¨ div.result-op æˆ– div.result ä¸­
        # æ ‡é¢˜é€šå¸¸æ˜¯ h3.news-title_1YtI1 (ç±»åå¯èƒ½ä¼šå˜ï¼Œç”¨ regex æ¨¡ç³ŠåŒ¹é…æˆ–æ‰¾ h3)
        news_items = soup.select('div.result-op, div.result')
        
        results = []
        for item in news_items[:5]:
            # æŸ¥æ‰¾ h3 æ ‡ç­¾ä½œä¸ºæ ‡é¢˜
            h3_tag = item.find('h3')
            if not h3_tag: continue
            
            link_tag = h3_tag.find('a')
            if not link_tag: continue
            
            title = link_tag.get_text().strip()
            link = link_tag['href']
            
            # æŸ¥æ‰¾å‘å¸ƒæ—¶é—´ (é€šå¸¸åœ¨ span.c-color-gray2 ä¸­)
            date_tag = item.select_one('.c-color-gray2')
            date = date_tag.get_text().strip() if date_tag else "è¿‘æœŸ"
            
            results.append({
                "title": title,
                "link": link,
                "date": date,
                "source": "Baidu"
            })
        return results
    except Exception as e:
        print(f"[Baidu] æŠ“å–å¤±è´¥: {e}")
        return []

def generate_markdown(data_dict):
    md = f"# ğŸŒ å…¨ç½‘æ•™è‚²ä¸AIèµ„è®¯æ—¥æŠ¥\n\n"
    md += f"**æ›´æ–°æ—¶é—´**: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')} (UTC)\n\n"
    
    for keyword, items in data_dict.items():
        md += f"## ğŸ“Œ {keyword}\n\n"
        if not items:
            md += "ä»Šæ—¥æš‚æ— ç›¸å…³æ–°é—»ã€‚\n\n"
            continue
            
        md += "| æ¥æº | æ ‡é¢˜ | æ—¶é—´ |\n"
        md += "|---|---|---|\n"
        
        # ç®€å•å»é‡ (é˜²æ­¢ä¸åŒå¼•æ“æœåˆ°åŒä¸€ç¯‡æ–‡ç« )
        seen_titles = set()
        
        for news in items:
            # æå–æ ‡é¢˜å‰10ä¸ªå­—ä½œä¸ºå»é‡æŒ‡çº¹
            title_fingerprint = news['title'][:10]
            if title_fingerprint in seen_titles:
                continue
            seen_titles.add(title_fingerprint)
            
            # æ ¼å¼åŒ–æ¥æºå›¾æ ‡
            icon = ""
            if news['source'] == "Google": icon = "ğŸ”µ G"
            elif news['source'] == "Bing": icon = "ğŸŸ¢ B"
            elif news['source'] == "Baidu": icon = "ğŸ”´ D"
            
            # æ¸…æ´—æ ‡é¢˜ä¸­çš„ç®¡é“ç¬¦ï¼Œé˜²æ­¢è¡¨æ ¼é”™ä¹±
            clean_title = news['title'].replace('|', '-').replace('\n', '')
            
            md += f"| {icon} | [{clean_title}]({news['link']}) | {news['date']} |\n"
        md += "\n"
        
    return md

def main():
    all_news = {}
    
    for keyword in KEYWORDS:
        print(f"\næ­£åœ¨æœç´¢å…³é”®è¯: {keyword} ...")
        
        # 1. æŠ“å– Google
        g_res = fetch_google_rss(keyword)
        time.sleep(1) # ç¤¼è²Œæ€§å»¶æ—¶
        
        # 2. æŠ“å– Bing
        b_res = fetch_bing_rss(keyword)
        time.sleep(1)
        
        # 3. æŠ“å– Baidu
        d_res = fetch_baidu_html(keyword)
        
        # åˆå¹¶ç»“æœ
        combined = g_res + b_res + d_res
        all_news[keyword] = combined
        
        print(f"  - Google: {len(g_res)}æ¡, Bing: {len(b_res)}æ¡, Baidu: {len(d_res)}æ¡")
        # å†æ¬¡å»¶æ—¶ï¼Œé˜²æ­¢è¿ç»­è¯·æ±‚ä¸åŒå…³é”®è¯å¯¼è‡´ IP è¢«å°
        time.sleep(random.uniform(2, 5)) 

    # ç”Ÿæˆ Markdown
    content = generate_markdown(all_news)
    
    with open("README.md", "w", encoding="utf-8") as f:
        f.write(content)
    print("\nREADME.md æ›´æ–°æˆåŠŸï¼")

if __name__ == "__main__":
    main()
